## 人工智能写词机

#### 项目简介：

以《全宋词》为数据，应用TensorFlow框架和LSTM模型，训练出可以针对给定词牌写出完整的一首词的人工智能写词机。

- 代码地址：https://github.com/koala0qoo/quiz-w11
- 模型地址：https://www.tinymind.com/yh001/quiz-w11-2
- 运行地址：https://www.tinymind.com/executions/ajyrcf84

#### 文件内容：

- embedding文件夹
-- word2vec_basic_ch.py 执行embedding操作的脚本
-- word2vec_basic_ch.ipynb 对代码的分析
-- tsne.png 对embedding结果的可视化
- QuanSongCi.txt 《全宋词》文本
- flags.py 命令行参数处理
- utils.py 读取与生成训练数据
- model.py 模型定义
- train.py 训练脚本
- sample.py 用最近的checkpoint，对三个词牌进行生成操作
- train_eval.py 执行训练与验证（词牌生成）过程
- log.png 输出内容截图


#### 结果分析：

输出结果

![输入图片说明](https://github.com/koala0qoo/img/blob/master/log.png?raw=true)


#### 对RNN的理解：

RNN：

传统DNN中，输入的每一条数据间是没有关联的，即只考虑当前一个状态。而RNN的特点是考虑一个连续的状态序列，每一条数据的输出不再独立，而是依赖于与他相邻的一条或多条数据的输入或输出，相当于赋予网络一定的记忆功能。

这种记忆功能是通过每个状态的隐藏层实现的。每一条输入信息都会转化为“隐藏状态”并在状态序列的隐藏层中向后传递。也就是说，每个时刻的输入由上一时刻的隐藏状态和本时刻的输入信息共同决定。理论上，每一时刻的隐藏状态可以一直向后传递，相当于每个时刻都可以捕获到前面所有时刻的信息。

这种负责传递记忆的隐藏层可以有多个，本作业中设置了3个。

LSTM：

上文提到，理论上隐藏状态可以一直向后传播。但实际上，如果依赖较长，在进行反向传播时容易发生梯度消失，导致权值无法更新。或者说，这样的网络是无法记住较长时间以前的信息的。而 LSTM（Long Short Term Memory）作为对传统RNN的改进，其主要目的是让网络可以拥有“较长的短期记忆”（不是记的时间长，而是记的内容多）。

LSTM 在传统 RNN 的隐藏状态的基础上增加了一个“细胞状态”，用于记忆的储存和传递。RNN 中用于传递记忆的是隐藏状态 h，而 h 每次向下一个时刻传递时都会乘以同一个矩阵 W（也是产生梯度消失或爆炸的原因）；LSTM 中的细胞状态 c 传递给下一个时刻的信息是上一时刻的信息和本时刻新输入信息的加和，因此不会受到矩阵连乘的影响。

但是，由于并不是所有的信息都是有意义的，LSTM 通过一个“遗忘门”控制要遗忘哪些之前的信息，通过“输入门”控制要记住哪些当前输入的信息，而最后的输出则通过“输出门”筛选后得到。这些门都以 sigmoid 函数的形式起作用。不同于权重矩阵，sigmoid 产生的是一个遗忘的概率，在 0-1 之间，接近 0 倾向于遗忘，接近 1 则倾向于记住。这样就保证了有用信息可以长时间传递。
